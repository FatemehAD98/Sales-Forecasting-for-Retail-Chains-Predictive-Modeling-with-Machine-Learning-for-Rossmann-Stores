{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ff5Bc5YDTF9K"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint , EarlyStopping\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C3iSPrfKTGPQ",
    "outputId": "3e79af93-bf3d-449c-e2a9-3102acf67ef6"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train.csv\",dtype={'StateHoliday': object})\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "store_df = pd.read_csv(\"store.csv\")\n",
    "\n",
    "train_df = pd.merge(train_df, store_df, how = 'left', on = 'Store')\n",
    "test_df = pd.merge(test_df, store_df, how = 'left', on = 'Store')\n",
    "\n",
    "ID = test_df['Id']\n",
    "test_df.drop('Id',inplace=True,axis=1)\n",
    "\n",
    "train_df.sort_values([\"Store\",\"Date\"], ignore_index=True, inplace=True)\n",
    "test_df.sort_values([\"Store\",\"Date\"], ignore_index=True, inplace=True)\n",
    "\n",
    "for dataset in (train_df,test_df):\n",
    "    dataset['Date'] = pd.to_datetime(dataset['Date'])\n",
    "    dataset['Day'] = dataset.Date.dt.day\n",
    "    dataset['Month'] = dataset.Date.dt.month\n",
    "    dataset['Year'] = dataset.Date.dt.year\n",
    "    dataset['DayOfYear'] = dataset.Date.dt.dayofyear\n",
    "    dataset['WeekOfYear'] = dataset.Date.dt.weekofyear\n",
    "    dataset.set_index('Date', inplace=True)\n",
    "\n",
    "\n",
    "store_data_sales = train_df.groupby([train_df['Store']])['Sales'].sum()\n",
    "store_data_customers = train_df.groupby([train_df['Store']])['Customers'].sum()\n",
    "store_data_avg_sales = train_df.groupby([train_df['Store']])['Sales'].mean()\n",
    "store_data_avg_customers = train_df.groupby([train_df['Store']])['Customers'].mean()\n",
    "store_data_open = train_df.groupby([train_df['Store']])['Open'].count()\n",
    "\n",
    "store_data_sales_per_day = store_data_sales / store_data_open\n",
    "store_data_customers_per_day = store_data_customers / store_data_open\n",
    "store_data_avg_sales_per_customer = store_data_avg_sales / store_data_avg_customers\n",
    "store_data_sales_per_customer_per_day = store_data_sales_per_day / store_data_customers_per_day\n",
    "\n",
    "sales_per_day_dict = dict(store_data_sales_per_day)\n",
    "customers_per_day_dict = dict(store_data_customers_per_day)\n",
    "avg_sales_per_customer_dict = dict(store_data_avg_sales_per_customer)\n",
    "sales_per_customers_per_day_dict = dict(store_data_sales_per_customer_per_day)\n",
    "\n",
    "\n",
    "\n",
    "train_df['SalesPerDay'] = train_df['Store'].map(sales_per_day_dict)\n",
    "train_df['Customers_per_day'] = train_df['Store'].map(customers_per_day_dict)\n",
    "train_df['Avg_Sales_per_Customer'] = train_df['Store'].map(avg_sales_per_customer_dict)\n",
    "train_df['Sales_Per_Customers_Per_Day'] = train_df['Store'].map(sales_per_customers_per_day_dict)\n",
    "\n",
    "test_df['Sales_per_day'] = test_df['Store'].map(sales_per_day_dict)\n",
    "test_df['Customers_per_day'] = test_df['Store'].map(customers_per_day_dict)\n",
    "test_df['Avg_Sales_per_Customer'] = test_df['Store'].map(avg_sales_per_customer_dict)\n",
    "test_df['Sales_Per_Customers_Per_Day'] = test_df['Store'].map(sales_per_customers_per_day_dict)\n",
    "\n",
    "\n",
    "freq2_dict_no_log = dict()\n",
    "freq3_dict_no_log = dict()\n",
    "\n",
    "amp2_dict_no_log = dict()\n",
    "amp3_dict_no_log = dict()\n",
    "\n",
    "\n",
    "for feat_1 in ('Year','Month'):\n",
    "        for i in range(min(train_df[feat_1].unique()), max(train_df[feat_1].unique()) + 1):\n",
    "\n",
    "            a = train_df.loc[train_df[feat_1]==i]\n",
    "            a_sales = a['Sales']\n",
    "\n",
    "            Y = np.fft.fft(a_sales.values)\n",
    "            Y = abs(Y)\n",
    "            freq = np.fft.fftfreq(len(Y), 1)\n",
    "\n",
    "            intercept_index = np.argmax(Y)\n",
    "            Y = np.delete(Y, intercept_index)\n",
    "            freq = np.delete(freq, intercept_index)\n",
    "\n",
    "            amplitude_1_index = np.argmax(Y)\n",
    "            amplitude_1 = Y[amplitude_1_index]\n",
    "            Y = np.delete(Y, amplitude_1_index)\n",
    "            freq_1 = freq[amplitude_1_index]\n",
    "            freq = np.delete(freq, amplitude_1_index)\n",
    "\n",
    "            amplitude_2_index = np.argmax(Y)\n",
    "            amplitude_2 = Y[amplitude_2_index]\n",
    "            Y = np.delete(Y, amplitude_2_index)\n",
    "            freq_2 = freq[amplitude_2_index]\n",
    "            freq = np.delete(freq, amplitude_2_index)\n",
    "\n",
    "            amplitude_3_index = np.argmax(Y)\n",
    "            amplitude_3 = Y[amplitude_3_index]\n",
    "            Y = np.delete(Y, amplitude_3_index)\n",
    "            freq_3 = freq[amplitude_3_index]\n",
    "            freq = np.delete(freq, amplitude_3_index)\n",
    "\n",
    "            a[f'Frequency_2_{feat_1}_Sales'] = freq_2\n",
    "            a[f'Frequency_3_{feat_1}_Sales'] = freq_3\n",
    "\n",
    "            a[f'Amplitude_2_{feat_1}_Sales'] = amplitude_2\n",
    "            a[f'Amplitude_3_{feat_1}_Sales'] = amplitude_3\n",
    "\n",
    "            freq2_dict_no_log[i] = freq_2\n",
    "            freq3_dict_no_log[i] = freq_3\n",
    "\n",
    "            amp2_dict_no_log[i] = amplitude_2\n",
    "            amp3_dict_no_log[i] = amplitude_3\n",
    "\n",
    "\n",
    "            if i == min(train_df[feat_1].unique()):\n",
    "                k = a\n",
    "            else:\n",
    "                k = pd.concat([k,a])\n",
    "        train_df = k\n",
    "        test_df[f'Frequency_2_{feat_1}_Sales'] = test_df[feat_1].map(freq2_dict_no_log)\n",
    "        test_df[f'Frequency_3_{feat_1}_Sales'] = test_df[feat_1].map(freq3_dict_no_log)\n",
    "        test_df[f'Amplitude_2_{feat_1}_Sales'] = test_df[feat_1].map(amp2_dict_no_log)\n",
    "        test_df[f'Amplitude_3_{feat_1}_Sales'] = test_df[feat_1].map(amp3_dict_no_log)\n",
    "        freq2_dict_no_log = dict()\n",
    "        freq3_dict_no_log = dict()\n",
    "        amp2_dict_no_log = dict()\n",
    "        amp3_dict_no_log = dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DaLfnRK9TQ0H"
   },
   "outputs": [],
   "source": [
    "feats = ['CompetitionOpenSinceMonth','CompetitionOpenSinceYear']\n",
    "modes = train_df[feats].mode()\n",
    "\n",
    "for f in feats:\n",
    "        train_df[f] = train_df[f].fillna(modes[f][0])\n",
    "        test_df[f] = test_df[f].fillna(modes[f][0])\n",
    "\n",
    "def convertCompetitionOpen(df):\n",
    "    try:\n",
    "        date = '{}-{}'.format(int(df['CompetitionOpenSinceYear']), int(df['CompetitionOpenSinceMonth']))\n",
    "        return pd.to_datetime(date)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "train_df['CompetitionOpenInt'] = train_df.apply(lambda df: convertCompetitionOpen(df), axis=1).astype(np.int64)\n",
    "test_df['CompetitionOpenInt'] = test_df.apply(lambda df: convertCompetitionOpen(df), axis=1).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WtfMGDExTTqf"
   },
   "outputs": [],
   "source": [
    "train_df.drop('Customers',inplace = True, axis=1)  #Because it is not in the test set\n",
    "train_df.drop('StateHoliday',inplace=True,axis=1)  #Because it reduces the performance\n",
    "test_df.drop('StateHoliday',inplace=True,axis=1)\n",
    "\n",
    "train_df.sort_values([\"Store\"], ignore_index=True, inplace=True)\n",
    "test_df.sort_values([\"Store\"], ignore_index=True, inplace=True)\n",
    "train_df.sort_values([\"Year\",\"Month\",\"Day\"], ascending=False ,ignore_index=True, inplace=True)\n",
    "test_df.sort_values([\"Year\",\"Month\",\"Day\"], ascending=False ,ignore_index=True, inplace=True)\n",
    "\n",
    "feats = ['Promo2SinceYear','Promo2SinceWeek','CompetitionDistance', 'PromoInterval']\n",
    "modes = train_df[feats].mode()\n",
    "\n",
    "for f in feats:\n",
    "        train_df[f] = train_df[f].fillna(modes[f][0])\n",
    "        test_df[f] = test_df[f].fillna(modes[f][0])\n",
    "for dataset in (train_df,test_df):\n",
    "    dataset['Open'] = dataset['Open'].fillna(0)\n",
    "\n",
    "attributes = ['StoreType','Assortment','PromoInterval']\n",
    "for dataset in (train_df,test_df):\n",
    "    for f in attributes:\n",
    "        dataset[attributes] = dataset[attributes].apply(lambda x: pd.factorize(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SF1N4fqGTgDb"
   },
   "outputs": [],
   "source": [
    "train_df = train_df[train_df['Open'] == 1]\n",
    "train_df = train_df[train_df['Sales'] > 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VqTVweq0Tkzm"
   },
   "outputs": [],
   "source": [
    "temp = train_df.sort_values([\"Year\", \"Month\", \"Day\"], ignore_index=True).copy()\n",
    "\n",
    "train = temp[:-47000].copy()\n",
    "test = temp[-47000:].copy()\n",
    "\n",
    "train.sort_values([\"Store\"], ignore_index=True, inplace=True)\n",
    "test.sort_values([\"Store\"], ignore_index=True, inplace=True)\n",
    "train.sort_values([\"Year\", \"Month\", \"Day\"], ascending=False, ignore_index=True, inplace=True)\n",
    "test.sort_values([\"Year\", \"Month\", \"Day\"], ascending=False, ignore_index=True, inplace=True)\n",
    "\n",
    "X = train.drop('Sales', axis=1)\n",
    "y = train['Sales']\n",
    "\n",
    "scaler_X = MinMaxScaler()\n",
    "X_normalized = scaler_X.fit_transform(X)\n",
    "scaler_y = MinMaxScaler()\n",
    "y_normalized = scaler_y.fit_transform(y.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_normalized, y_normalized, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mpONPr7eEah1",
    "outputId": "0df7e997-992a-4452-803d-92b5f429a194"
   },
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7DHNxJ7A0rBZ"
   },
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uiTw4wX1YXFy"
   },
   "outputs": [],
   "source": [
    "def rmspe_loss(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, dtype=tf.float32)\n",
    "    y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
    "\n",
    "    error = tf.math.divide_no_nan(y_true - y_pred, y_true)\n",
    "    squared_error = tf.square(error)\n",
    "    mean_squared_error = tf.reduce_mean(squared_error)\n",
    "    rmspe = tf.sqrt(mean_squared_error)\n",
    "    return rmspe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_units = 64\n",
    "input_layer = layers.Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "lstm_output = layers.LSTM(units=lstm_units, activation='tanh', return_sequences=True)(input_layer)\n",
    "\n",
    "cnn_filters = 32\n",
    "cnn_kernel_size = 3\n",
    "cnn_output = layers.Conv1D(filters=cnn_filters, kernel_size=cnn_kernel_size, padding='same', activation='relu')(lstm_output)\n",
    "\n",
    "attention_units = 64\n",
    "attention_output = layers.MultiHeadAttention(num_heads=2, key_dim=attention_units)(cnn_output, cnn_output)\n",
    "\n",
    "global_avg_pooling = layers.GlobalAveragePooling1D()(attention_output)\n",
    "\n",
    "dense_units = 128\n",
    "dense_output = layers.Dense(units=dense_units, activation='relu')(global_avg_pooling)\n",
    "\n",
    "output_layer = layers.Dense(units=1, activation='linear')(dense_output)\n",
    "\n",
    "model_1 = models.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "model_1.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), loss=\"mse\", metrics=[rmspe_loss])\n",
    "\n",
    "model_1.summary()\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath='lstm_cnn_trans.h5',\n",
    "    monitor='val_loss',  \n",
    "    save_best_only=True,  \n",
    "    save_weights_only=True, \n",
    "    mode='min'  \n",
    ")\n",
    "\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_loss',  \n",
    "    patience=5,  \n",
    "    restore_best_weights=True \n",
    ")\n",
    "\n",
    "history_1 = model_1.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test),callbacks=[checkpoint_callback,early_stopping_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history_1.history['loss']\n",
    "val_loss = history_1.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.plot(epochs, loss, label='Training Loss')\n",
    "plt.plot(epochs, val_loss, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMultiHeadAttention(layers.MultiHeadAttention):\n",
    "    def call(self, inputs, **kwargs):\n",
    "        value = inputs\n",
    "        return super().call(inputs, value=value, **kwargs)\n",
    "\n",
    "total_training_samples = 637870\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "\n",
    "total_steps = (total_training_samples / batch_size) * epochs\n",
    "\n",
    "decay_steps = int(0.1 * total_steps)\n",
    "\n",
    "initial_learning_rate = 3e-4\n",
    "lr_schedule = ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=decay_steps,\n",
    "    decay_rate=0.9,\n",
    "    staircase=True)\n",
    "\n",
    "model_2 = tf.keras.Sequential([\n",
    "    layers.Input(shape=(X_train.shape[1], 1)),\n",
    "    tf.compat.v1.keras.layers.CuDNNLSTM(64, return_sequences=True),\n",
    "    CustomMultiHeadAttention(num_heads=2, key_dim=2),\n",
    "    layers.GlobalAveragePooling1D(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "model_2.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule), loss=rmspe_loss, metrics=[rmspe_loss])\n",
    "history_2 = model_2.fit(X_train, y_train, epochs=100, batch_size=64, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history_2.history['loss']\n",
    "val_loss = history_2.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.plot(epochs, loss, label='Training Loss')\n",
    "plt.plot(epochs, val_loss, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=test.copy()\n",
    "X_new_testing = df_test.drop([\"Sales\"],axis=1)\n",
    "x_new_testing_normalized = scaler_X.transform(X_new_testing)\n",
    "x_new_testing_normalized = x_new_testing_normalized.reshape(x_new_testing_normalized.shape[0], x_new_testing_normalized.shape[1], 1)\n",
    "y_new_testing = df_test[\"Sales\"]\n",
    "y_new_testing_normalized = scaler_y.transform(np.array(y_new_testing).reshape(-1, 1))\n",
    "\n",
    "model_1.evaluate(x_new_testing_normalized, y_new_testing_normalized), model_2.evaluate(x_new_testing_normalized, y_new_testing_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_normalized = scaler_X.transform(test_df)\n",
    "y1 = model_1.predict(test_df_normalized)\n",
    "y2 = model_2.predict(test_df_normalized)\n",
    "\n",
    "y_preds = (y1+y2)/2\n",
    "\n",
    "y_preds_denorm = scaler_y.inverse_transform(y_preds)\n",
    "\n",
    "submission_1 = pd.DataFrame()\n",
    "\n",
    "submission_1['Id'] = range(1, len(y_preds) + 1)\n",
    "submission_1['Sales'] = y_preds_denorm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_1.to_csv('ensemble.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
